---
title: "Tarea3"
author: "Sara, Valeria, Ivan, Alberto"
format: pdf
editor: source
---

# Tarea 3.

```{r message=FALSE}
library(ggplot2)
library(tidyverse)
library(patchwork)
library(kableExtra)
```

1.  Calcular el estimador de Monte Carlo de la integral $$\int_0^{\pi/3}\text{sin}(t)dt$$ y comparar el estimador con el valor exacto de la integral.
```{r}
pi_tercios = pi/3

getMCArea <- function(f,xinf,xsup,n){

    # Generamos n puntos aleatorios 
    x <- runif(n, xinf, xsup) # [0, 1]
    y <- runif(n, 0, max(f(x)))  # [0, max(f)]

    # Calculamos los puntos que caen debajo de la curva y en ella
    points_in_curve <- sum(y <= f(x))

    # Calculamos la proporción de puntos dentro de la curva respecto al 
    # total de puntos lanzados
    proportion <- points_in_curve / n

    # Calculamos el área total del rectángulo que cubre la función f de integración
    total_area <- (xsup - xinf) * max(f(x))

    # Multiplicamos la proporción de puntos debajo de la curva 
    #por el área total del rectángulo
    integral_estimate <- proportion * total_area

    return(integral_estimate) 
} 

# Definimos la función a integrar
f_sin <- function(x) {
  sin(x)
}

#número máximo de puntos aleatorois para el cálculo
n <- 100000

#limites
xinf = 0
xsup=pi_tercios

# Resultado
integral_estimate <- getMCArea(f_sin,xinf,xsup,n)
cat("El valor exacto de la integral es 0.5; 0.1 % diferente al valor estimado: ", integral_estimate, "\n")

```

2.  Escribir una función para calcular el estimador de Monte Carlo de la función de distribución $\mathcal{Be}(3,3)$ y usar la función para estimar $F(x)$ para $x = 0.1,...,0.9$. Comparar los estimados con los valores obtenidos con la función *pbeta* de *R*.

```{r}

# Definir la función para calcular el estimador de Monte Carlo de F(x) para una distribución beta Be(3, 3)
monte_carlo_estimator <- function(x, n = 100000) {
  # Generar n números aleatorios que sigan una distribución beta Be(3, 3)
  random_values <- rbeta(n, 3, 3)
  # Calcular la fracción de valores generados que son menores o iguales a x
  monte_carlo_estimate <- mean(random_values <= x)
  return(monte_carlo_estimate)
}

# Calcular los estimados de Monte Carlo de F(x) para x = 0.1, 0.2, ..., 0.9
x_values <- seq(0.1, 0.9, by = 0.1)
monte_carlo_estimates <- sapply(x_values, monte_carlo_estimator)

# Calcular los valores de referencia de F(x) utilizando la función pbeta de R
reference_values <- pbeta(x_values, 3, 3)

# Mostrar los resultados
results <- data.frame(x = x_values, Estimador_MC = monte_carlo_estimates, PBETA = reference_values)
print(results)

`````

\newpage

## 3. Usar integración Monte Carlo para estimar: $$\int_0^1\frac{e^{-x}}{1+x^2}dx$$

```{r}

getMCArea <- function(f,xinf,xsup,n){

    # Generamos n puntos aleatorios 
    x <- runif(n, xinf, xsup) # [0, 1]
    y <- runif(n, 0, max(f(x)))  # [0, max(f)]

    # Calculamos los puntos que caen debajo de la curva y en ella
    points_in_curve <- sum(y <= f(x))

    # Calculamos la proporción de puntos dentro de la curva respecto al 
    # total de puntos lanzados
    proportion <- points_in_curve / n

    # Calculamos el área total del rectángulo que cubre la función f de integración
    total_area <- (xsup - xinf) * max(f(x))

    # Multiplicamos la proporción de puntos debajo de la curva 
    #por el área total del rectángulo
    integral_estimate <- proportion * total_area

    return(integral_estimate) 
} 

```

```{r}

# Definimos la función a integrar
f <- function(x) {
  exp(-x) / (1 + x^2)
}

#número máximo de puntos aleatorois para el cálculo
n <- 1000000

#limites
xinf = 0
xsup=1

# Resultado
integral_estimate <- getMCArea(f,xinf,xsup,n)
cat("Estimación de la integral de f(x) mediante el MC:", integral_estimate, "\n")


```

calcular el tamaño de muestra necesario para obtener un error de estimación máximo de $\pm 0.001$.

```{r}
 
# Calculamos el valor exacto de la integral
exact_integral <- integrate(f, 0, 1)$value;

# Definimos el error de estimación máximo
max_error <- 0.001;

# Inicializar el tamaño de muestra
n <- 0;
current_error <- 1;

while (current_error > max_error) {
  n <- n + 1;
  curr_area <- getMCArea(f,xinf,xsup,n)
  current_error <- abs( curr_area - exact_integral);
  
}

cat("Tamaño de muestra necesario con max error de +-0.001$ = "
    , n, ",con área = ", curr_area)

 
```

\newpage

## 4. Sea $\hat\theta_{IS}$ el estimador de importancia de $\theta=\int g(x)dx$, donde la función de mportancia $f$ es una densidad. Probar que si $g(x)/f(x)$ es acotada, entonces la varianza del estimador de muestreo por importancia $\hat\sigma_{IS}$ es infinita.

#### Demostración Analítica:

Recordando la definición de la varianza del estimador de muestreo por importancia:

-   $\hat{\sigma}_{IS}^2 = \text{Var}(\hat{\theta}{IS}) = E[(\hat{\theta}_{IS} - \theta)^2]$

Donde:

-   $\hat{\theta}_{IS}$ es el estimador de importancia de $\theta.$

-   $\theta$ es el valor verdadero que estamos tratando de estimar (en este caso, $\int g(x)$). $E[\cdot]$ es el valor esperado.

Ahora definamos $W(x) = \frac{g(x)}{f(x)}$. Podemos, entonces, definir el estimador de importancia $\hat{\theta}_{IS}$ *como:* $\hat{\theta}_{IS}$ = $\int W(x) f(x) dx$

Para demostrar que la varianza del estimador de muestreo por importancia $\hat{\theta}_{IS}$ es finita, mostraremos que $E[\hat{\theta}_{IS}]$ y $E[\hat{\theta}_{IS}^2]$ son finitos.

1.  Finitud de $E[\hat{\theta}_{IS}]$:
    -   $E[\hat{\theta}_{IS}]$ = $E[\int W(x) f(x) dx]$ = $\int E[W(x)] f(x) dx ]$

Dado que $\frac{g(x)}{f(x)}$ está acotada, $W(x)$ es una función acotada y, por lo tanto, $E[W(x)]$ es finito. Por lo tanto, la integral anterior es finita, lo que implica que $E[\hat{\theta}_{IS}]$ es finito.

2.  Finitud de $E[\hat{\theta}_{IS}^2]$:
    -   $E[\hat{\theta}_{IS}^2]$ = $E[(\int W(x) f(x) dx)^2]$ = $\iint W(x_1)W(x_2) f(x_1)f(x_2) dx_1dx_2$

Dado que $\frac{g(x)}{f(x)}$ está acotada, $W(x)$ es una función acotada, y por lo tanto $W(x_1)W(x_2)$ es acotada para todos los $x_1$ y $x_2$. Por lo tanto, la integral anterior es finita, lo que implica que $E[\hat{\theta}_{IS}^2]$ es finito.

Dado que tanto $E[\hat{\theta}_{IS}]$ como $E[\hat{\theta}_{IS}^2]$ son finitos, la varianza $\hat{\sigma}_{IS}^2$ también es finita.

#### Implementación en R:

Vamos a generar funciones $f(x)$ y $g(x)$ arbitrarias que estén acotadas, y luego realizaremos una simulación para calcular la varianza del estimador.

```{r}
 
# Función f(x) (densidad de importancia) - podemos usar una distribución uniforme
f <- function(x) {
  return(dunif(x, min = 0, max = 1))  # Distribución uniforme entre 0 y 1
}

# Función g(x) - podemos usar una función acotada arbitraria
g <- function(x) {
  return(x^2)  # Por ejemplo, una función cuadrática acotada en [0, 1]
}

# Número de muestras
N <- 10000

# Generar muestras utilizando la densidad de importancia f(x)
samples <- runif(N, min = 0, max = 1)

# Calcular el estimador de importancia theta_IS
theta_IS <- mean(g(samples))

# Calcular la varianza del estimador
variance <- var(g(samples))

# Verificar si la varianza es finita
if (is.finite(variance)) {
  print("La varianza del estimador de muestreo por importancia es finita.")
} else {
  print("La varianza del estimador de muestreo por importancia no es finita.")
}


```

## 5. Encontrar dos funciones de importancia $f_1$ y $f_2$ que tengan soporte en $(1,\infty)$ y estén 'cerca' de: $$g(x)=\frac{x^2}{\sqrt{2\pi}}e^{x^2/2}, x>1$$

¿Cuál de las dos funciones de importancia debe producir la varianza más pequeña para estimar la integral siguiente por muestreo de importancia?

$$\int_1^\infty\frac{x^2}{\sqrt{2\pi}}e^{x^2/2}$$

6.  Usar el algoritmo de Metropolis-Hastings para generar variadas aleatorias de una densidad Cauchy estándar. Descarta las primeras 1000 observaciones de la cadena, y comparar los deciles de las observaciones generadas con los deciles de la distribución Cauchy estándar. Recordar que una densidad $\text{Cauchy}(\theta,\eta)$ tiene densidad dada por la siguiente función:

$$f(x)=\frac{1}{\theta\pi\Big(1+\Big[\frac{x-\eta}{\theta}\Big]^2\Big)}, x\in\Re,\theta>0$$

La densidad Cauchy tiene $\theta=1, \eta=0$, y corresponden a la densidad $t$ con un grado de libertad.

7.  Implementar un muestreador de Metropolis de caminata aleatoria para generar muestras de una distribución estándar de Laplace: $$f(x)=\frac{1}{2}e^{-|x|}, x\in\Re$$ Para el incremento, simula una normal estándar. Comparar las cadenas generadas cuando la distribución propuesta tiene diferentes varianzas. Calcular las tasas de aceptación de cada cadena.

8.  Desarrollar un algoritmo de Metropolis-Hastings para muestrear de la distribución siguiente:

```{r echo=FALSE}
tabla = tibble(dado = c(1,2,3,4,5,6), probabilidad = c(0.01,0.39,0.11,0.18,0.26,0.05))
tabla %>% kable() %>% kable_paper(full_width = FALSE)
```

con la distribución propuesta basada en un dado honesto.

9.  La sucesión de Fibonacci 1, 1, 2, 3, 5, 8, 13,... es descrita por recurrencia $f_n=f_{n-1}+f_{n-2}$, para $n\geq 3$ con $f_1=f_2=1$

<!-- -->

a.  Mostrar que el número de sucesiones binarias de longitud $m$ sin 1's adyacentes es $f_{m+2}$

b.  Sea $p_{k,m}$ el número de sucesiones binarias de longitud $m$ con exactamente $k$ 1's. Mostrar que $$p_{k,m}=\binom{m-k+1}{k}, k=0,1,...,\text{ceiling}(m/2)$$
